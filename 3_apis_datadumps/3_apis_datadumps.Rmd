---
title: "3 APIs and data dumps"
output: html_notebook
author: Dr James Tripp, CIM, University of Warwick
---

# Overview

Structured, table like data is available for most social media platforms. We can access this data by sending requests to the platform website specifying the data we want. A major advantage of this approach is that we can get a lot of easy to use data. However, we are also dependent on the platform and the limits they impose on data collection.

# Direct get requests

R has a library called [httr](https://github.com/r-lib/httr) which allows us to send a request to a web location. If we ask for a web page then we will get that page, which appears to us like garbage text.

```{r}
rm(list = ls())
library(httr)
GET('www.google.com')
```

But httr gets much more interesting once we realise that many APIs are just get requests with options to a particular webpage. 

# 4Chan

Let us consider the highly controversial message board 4chan. If we look at the documentation, 4chan has [a relatively simple API](https://github.com/4chan/4chan-API).

**NOTE:** 4chan has offensive content. Like, really, really, offensive content. Tread extremely carefully! Alternatively, use the Reddit example below instead.

Taken from the 4chan API page

'JSON representations of threads and indexes are exposed at the following URLs:

http(s)://a.4cdn.org/board/thread/threadnumber.json
http(s)://a.4cdn.org/board/pagenumber.json (1 is main index)'

Roughly translated, if we direct a get request to https://a.4cdn.org/g/1.json then we will recieve a JSON file containing the contents of the main page of the technology board. JSON is a structured text file and R can read this. In other words, we have hit data gold. Let us go ahead and download the data from the front page.

```{r}
GET('http://a.4cdn.org/g/1.json')
```

The 200 status means we were successful. Superb.

We are not going to print out the raw json here. Instead, we will ask R to parse the json file and put it into a tibble for us. After some exploration of the output, I noticed that the posts were located at $threads$posts.

```{r}
library(jsonlite)
library(dplyr)
x <- fromJSON('http://a.4cdn.org/g/1.json')$threads$posts
our_data <- bind_rows(x)
```

The above two lines downlaod the JSON file using a helpful function called fromJSON. The result is a list of data frames, not a data frame. The function bind_rows takes a list of data frames and outputs a single tibble.

The result looks ok.

```{r}
our_data
```

The comment section includes the HTML formatting. We can parse that to make the text more readable and replace the emoji characters with word equivilants. I have also gone through and replaced apostrophe characters.

```{r}
library(textclean)
library(stringr)
our_data$com <- replace_emoji(replace_html(our_data$com))
our_data$com <- str_replace_all(string = our_data$com, pattern = '&#039;', replacement = '\'')
our_data$com <- str_replace_all(string = our_data$com, pattern = '&#039;', replacement = '<e2><80><99>')
```

Now we have a data set which is looking a little better.

```{r echo=FALSE}
our_data
```

## As a function

We have code which will download 4chan posts. The code API, as we use it, has two parameters: board and page. We can create a function to which we supply the board and page, then the function returns the corresponding posts.

```{r 4chan_function}
get_4chan <- function(board, page) {
  url <- paste('http://a.4cdn.org/', board, '/', as.character(page), '.json', sep = '')
  x <- fromJSON(url)$threads$posts
  our_data <- bind_rows(x)
  our_data$com <- replace_emoji(replace_html(our_data$com))
  our_data$com <- str_replace_all(string = our_data$com, pattern = '&#039;', replacement = '\'')
  our_data$com <- str_replace_all(string = our_data$com, pattern = '&#039;', replacement = '<e2><80><99>')
  our_data$board <- board
  our_data
}
```

Notice that I transform the page number into a character. I do this so that the URL is all text and R does not give us an error. I have also added a board column so that we know which board the data came from - this will be useful in the future.

To use this function, we need to specify the board and the page.

```{r}
get_4chan(board = 'g', page = 3)
```

## Many comments

Our function allows us to download multiple pages of comments from multiple boards. **Note** We would need to write more code to get the comments of every thread. The data we are downloading is simple the first entry in every thread.

Let us get the first 20 pages of the technology(g), sports(sp) and fitness(fit) boards.

```{r}
boards <- c('g', 'sp', 'fit')
pages <- 1:10

our_data <- tibble()
for (board in boards) {
  for (page in pages) {
    our_data <- bind_rows(
      our_data,
      get_4chan(board, page)
    )
  }
}
```

We now have over 2000 comments from 4chan.

## Summary and visualisation

The data has lots of interesting columns.

```{r}
names(our_data)
```

Perhaps the easiest thing to do is to plot the number of replies.

```{r}
library(ggplot2)
ggplot(data = our_data, aes(x = board, y = replies)) +
  geom_boxplot()
```

Some small differences. What does the plot mean to you?

Perhaps we should do a wordcount too.

```{r}
library(tidyverse)
library(magrittr)
small_data <- our_data[,c('com', 'board')] %>% drop_na()
small_data %>%
  mutate(
    wordcount = str_count(com, pattern = '') + 1
    ) %>%
  group_by(board) %>%
  summarise(
    mean_wc = mean(wordcount)
  )


```

```{r}
our_data %>%
  mutate(
    wordcount = str_count(com, pattern = '') + 1
    )
```

We can follow the previous section using much the same code to generate a word could plot by board. The only differences are that we calculate the word count on the com column, group by board, remove any columns where there are missing values and only selected the columns we are using - in our cases these are in the com column.

```{r}
our_data[,c('com', 'board')] %>%
  na.omit() %>%
  mutate(
    wordcount = str_count(com, pattern = '') + 1
    ) %>%
  group_by(board) %>%
  summarise(
    median_wc = median(wordcount),
    max_wc = max(wordcount),
    min_wc = min(wordcount)
  ) %>%
  ggplot(aes(x = board, y = median_wc)) +
  geom_point() +
  geom_errorbar(aes(ymin = min_wc, ymax = max_wc)) +
  coord_flip() +
  theme_minimal()
```

# Reddit

Reddit, another social media site, offers json data files too!

The formal documentation for the API is available [here](https://www.reddit.com/dev/api/). But the API requires open authentication, which is a little trickier. Instead, we'll opt for the simpler approach of adding .json to page as [detailed here](https://www.reddit.com/r/redditdev/comments/6anc5z/what_on_earth_is_the_url_to_get_a_simple_json/).

Our code to access the front page of the [united kingdom reddit](https://www.reddit.com/r/unitedkingdom) is:

```{r}
base_url <- 'https://www.reddit.com/r/unitedkingdom'
url <- paste(base_url, '.json', sep = '')
reddit_data <- fromJSON(url)$data$children$data
names(reddit_data)
```

Admittedly, finding that the data is in $data$children$data took a little work. Let us plot the upvoted of the different entries.

```{r}
library(tibble)
my_tibble <- as_tibble(reddit_data)
my_tibble[,c( 'title', 'ups')] %>%
  na.omit() %>%
  mutate(short_title = substr(title, 1, 50)) %>% # make a shorter title so our plot isn't too big
  ggplot(aes(x = short_title, y = ups)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()
```

## RedditExtractoR

We could write a for loop which collect the data from the above reddit pages. However, someone has done a far better job than we would do. That person, Ivan Rivera, wrote the [RedditExtractoR package](https://github.com/ivan-rivera/RedditExtractoR).

Let's load in the package and see how to use it.

```{r}
library(RedditExtractoR)
?RedditExtractoR
```

Great, so run a search and then it returns a bunch of comment data.

```{r results='hide', message=FALSE}
query <- 'brexit'
our_data <- get_reddit(query, subreddit = 'unitedkingdom')
```

That will take a while. However, it works rather well.

# Other APIs

There are methods, in R, for accessing other social media APIs. Below is code for accessing the Twitter and YouTube APIs. To download data from these platforms you will need API keys:

* [Instructions for generating a YouTube API key](https://developers.google.com/youtube/v3/getting-started)
* [Twitter documentation about API tokens](https://developer.twitter.com/en/docs/basics/authentication/guides/access-tokens.html)
* [Guide for creating a Twitter API key](https://www.slickremix.com/docs/how-to-get-api-keys-and-tokens-for-twitter/)

## YouTube

The Google APIs are quite like the other JSON dumps above. We send a request to Google via a url containing the API key and our parameters. The below code will get a video description.

```{r eval=FALSE}
download_youtube_video_description <- function(video_id, api_key, part = 'snippet'){
  api_url <- 'https://www.googleapis.com/youtube/v3/videos'
  api_options <- list(
    part = part,
    key = api_key,
    id = video_id
  )
  response <- plyr::ldply(
    httr::content(httr::GET(api_url, query = api_options))$items,
    data.frame,
    stringsAsFactors = FALSE
  )
  data.frame(
    id = response$id,
    published = response$snippet.publishedAt,
    title = response$snippet.title,
    description = response$snippet.description,
    stringsAsFactors = FALSE
  )
}
```

Where you should provide the video id (the part of the URL which is after v=) and your API key.

**note** you will need to generate API keys for your use.

You can put this in a loop for multiple video descriptions

```{r eval=FALSE}
download_youtube_video_descriptions <- function(video_ids, api_key, part = 'snippet'){
  
  for (i in 1:length(video_ids)) {
    id <- video_ids[i]
    result <- rbind(
      result,
      download_youtube_video_description(video_id = id, api_key = api_key)
    )
  }
  
result
}
```

The above functions are slightly modified from the current version of the LE-CAT app I am developing with Dr Noortje Marres at CIM.

## Twitter

Twitter API access is best done using the excellent TwitteR package. The code below is taken from the superb [medium post by Michael Galarnyk](https://towardsdatascience.com/access-data-from-twitter-api-using-r-and-or-python-b8ac342d3efe) - well worth a read.

**note** you will need to generate API keys for your use.

```{r eval=FALSE}
#taken from 
#install.packages("twitteR")
library(twitteR) 
# Change consumer_key, consume_secret, access_token, and 
# access_secret based on your own keys
consumer_key <- ""
consumer_secret <-""
access_token <- ""
access_secret <- "" 
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tw = searchTwitter('@GalarnykMichael', n = 25)
d = twListToDF(tw)
```

# Summary

We have looked at how to collect data from 4chan and Reddit via JSON files. We have also skimmed over samples of code for accessing the YouTube and Twitter APIs. 

R allows us to load in these data and then create some basic summaries and plots. In the next segment, we will take a moment to look a little closer at the tidyverse verb based manipulation of tables to generate summaries.

**note** You may notice that instagram and facebook are missing from the above. Unfortunatly, the APIs for these platforms have been changed and are not very useful. There are few reliable package which allow you to download data from these platforms.

## Your turn

Collect some reddit data on a topic you are interested in. Choose a query and subreddits which are relevent to your interest. After collecting the data, save your data as an RData file using the save() function.

# Going further

If you want to go further with these techniques then the following might be useful:

* [httr vignette about API access](https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html)
* [RStudio Conference 2017 talk on accessing web APIs](https://www.rstudio.com/resources/videos/using-web-apis-from-r/)
* [Rstudio video on accessing web APIs](https://www.rstudio.com/resources/videos/using-web-apis-from-r/)
* The [rTweet](https://rtweet.info) package looks like a better way to access Twitter via R. However, I have not tried it before this workshop.

There are lots of different packages for collecting social media data. The [vosonSML](https://github.com/vosonlab/vosonSML) package is a front end to the [rtweet](https://github.com/mkearney/rtweet) and [RedditExtractoR](https://github.com/ivan-rivera/RedditExtractoR) packages. It appears to have some excellent network analysis component and is most certainly worth you looking into.

In addition:

* [DMI-TCAT](https://github.com/digitalmethodsinitiative/dmi-tcat/wiki) - A software package (not in R) which allows one to download and archive data Twitter data. The data can then be analysed using a web interface. We have several TCAT servers set up at CIM.
* [NetVizz](https://tools.digitalmethods.net/netvizz/facebook/netvizz/) - A Facebook APP for downloading data.
* [YouTube Data Tools](https://tools.digitalmethods.net/netvizz/youtube/) - A free app for downloading YouTube data. You can download the data and then load it into R using the command read.csv(filename, sep = '\t'). Where filename is the filename of your downloaded file.

